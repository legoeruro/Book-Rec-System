{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code for the XGBoost recommendation system. The limitations of this is:\n",
    "1. Take a lot of computing power and memory. The most data my laptop can run for this is 30000 ratings and 10000 books.\n",
    "\n",
    "*Tunable parameters:\n",
    "- Number of training instances\n",
    "- Input for the XGBoost model\n",
    "- What to use to encode the data: label, one-hot, or binary encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/legoeuro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from brs_data_preprocessing import get_preprocessed_data as preproc, merged_book_ratings as merge\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersDf, bookDf, ratingDf = preproc('Users.csv', 'Books.csv', 'Ratings.csv')\n",
    "\n",
    "#configs\n",
    "#Convert all strings to categorical (might not be good for title)\n",
    "bookDf['Book-Title'] = bookDf['Book-Title'].astype('category')\n",
    "bookDf['Book-Author'] = bookDf['Book-Author'].astype('category')\n",
    "bookDf['Publisher'] = bookDf['Publisher'].astype('category')\n",
    "\n",
    "ratingDf['ISBN'] = ratingDf['ISBN'].astype(str)\n",
    "bookDf['ISBN'] = bookDf[\"ISBN\"].astype(str)\n",
    "ratingDf['User-ID'] = ratingDf['User-ID'].astype(int)\n",
    "usersDf['User-ID'] = ratingDf['User-ID'].astype(int)\n",
    "\n",
    "ratingDf = ratingDf.iloc[:30000]\n",
    "bookDf = bookDf.iloc[:10000]\n",
    "\n",
    "# bookDf.drop(columns=['Book-Title'], inplace=True)\n",
    "\n",
    "COUNTRY_INDEX = 2\n",
    "CITY_INDEX = 1\n",
    "STATE_INDEX = 0\n",
    "#For now I will use only country data\n",
    "usersDf['Location'] = usersDf['Location'].apply(lambda x: x.split(','))\n",
    "usersDf.drop(usersDf[usersDf['Location'].apply(lambda x: len(x) != 3)].index, inplace=True)\n",
    "usersDf['Location'] = usersDf['Location'].apply(lambda x: x[CITY_INDEX])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary encoding (trade-off between data being ordered and memory cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "enc_author = BinaryEncoder(cols=['Book-Author'])\n",
    "bookDf = enc_author.fit_transform(bookDf)\n",
    "\n",
    "enc_publisher = BinaryEncoder(cols=['Publisher'])\n",
    "bookDf = enc_publisher.fit_transform(bookDf)\n",
    "\n",
    "enc_location = BinaryEncoder(cols=['Location'])\n",
    "usersDf = enc_location.fit_transform(usersDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other encoding methods are label encoding (bad since our data is not ordinal), and one-hot encoding (not good when I tested - inefficient in memory and computing power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# import nltk\n",
    "\n",
    "# for label encoding\n",
    "# bookDf['Book-Author'] = label_encoder.fit_transform(bookDf['Book-Author'])\n",
    "# bookDf['Publisher'] = label_encoder.fit_transform(bookDf['Publisher'])\n",
    "# usersDf['Location'] = label_encoder.fit_transform(usersDf['Location'])\n",
    "\n",
    "# for one hot encoding\n",
    "# authorCols = pd.get_dummies(bookDf['Book-Author'])\n",
    "# publisherCols = pd.get_dummies(bookDf['Publisher'])\n",
    "# bookDf = pd.concat([bookDf, authorCols, publisherCols], axis=1)\n",
    "# bookDf.drop(columns=['Book-Author', 'Publisher'], inplace=True)\n",
    "\n",
    "# locationCols = pd.get_dummies(usersDf['Location'])\n",
    "# usersDf = pd.concat([usersDf, locationCols], axis=1)\n",
    "# usersDf.drop(columns=['Location'], inplace=True)\n",
    "\n",
    "# for SVD on publisher, author. SVD for users is beslow\n",
    "# from testing on this dataset (truncated to 10000 books), SVD on publisher and author does not give better results\n",
    "# svd = TruncatedSVD(n_components=100)\n",
    "# authorSVD = CountVectorizer().fit_transform(bookDf['Book-Author'])\n",
    "\n",
    "# authorSVD = svd.fit_transform(authorSVD)\n",
    "# authorDF = pd.DataFrame(data=authorSVD).add_prefix('Book-Author-')\n",
    "# bookDf = pd.concat([bookDf, authorDF], axis=1)\n",
    "# bookDf.drop(columns=['Book-Author'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                     classical mythology\n",
      "1                                            clara callan\n",
      "2                                       decision normandy\n",
      "3       flu: story great influenza pandemic 1918 searc...\n",
      "4                                         mummies urumchi\n",
      "                              ...                        \n",
      "9995                 read tell says : stories (bard book)\n",
      "9996                                           star rover\n",
      "9997                                     die keltennadel.\n",
      "9998                                     tod der datscha.\n",
      "9999                                              dunkel.\n",
      "Name: Book-Title, Length: 10000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "def preprocess(column):\n",
    "\n",
    "    #make all words with lower letters\n",
    "    column = column.str.lower()\n",
    "    #getting rid of any punctution\n",
    "    column = column.str.replace('http\\S+|www.\\S+|@|%|:|,|', '', case=False)\n",
    "    #spliting each sentence to words to apply previous funtions on them \n",
    "    word_tokens = column.str.split()\n",
    "    keywords = word_tokens.apply(lambda x: [item for item in x if item not in stop])\n",
    "    #assemble words of each sentence again and assign them in new column\n",
    "    for i in range(len(keywords)):\n",
    "        keywords[i] = \" \".join(keywords[i])\n",
    "        column = keywords\n",
    "    return column\n",
    "\n",
    "bookDf['Book-Title'] = preprocess(bookDf['Book-Title'])\n",
    "print(bookDf['Book-Title'])\n",
    "bookVectorized = CountVectorizer().fit_transform(bookDf['Book-Title'])\n",
    "# bookDf = pd.concat([bookDf, pd.DataFrame(data=bookVectorized.toarray())], axis=1)\n",
    "# bookDf.drop(columns=['Book-Title'], inplace=True)\n",
    "\n",
    "#too much columns --> SVD\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "bookVectorized = svd.fit_transform(bookVectorized)\n",
    "titleDF = pd.DataFrame(data=bookVectorized).add_prefix('Book-Title-')\n",
    "bookDf = pd.concat([bookDf, titleDF], axis=1)\n",
    "bookDf.drop(columns=['Book-Title'], inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate input for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateInput(X_u, X_b, y):\n",
    "    \"\"\"\n",
    "    X_u: User features\n",
    "    X_b: Book features\n",
    "    y: Ratings\n",
    "    tgt_users: Target users\n",
    "    \"\"\"\n",
    "    merged = pd.merge(y, X_u, on='User-ID', how='inner')\n",
    "    merged = pd.merge(merged, X_b, on='ISBN', how='inner')\n",
    "\n",
    "    merged.drop(columns=['ISBN', 'User-ID'], inplace=True)\n",
    "\n",
    "    # merged.fillna(0, inplace=True)\n",
    "    merged = merged.dropna()\n",
    "    return (merged.drop(columns=['Book-Rating']), merged['Book-Rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Location_0  Location_1  Location_2  Location_3  Location_4  \\\n",
      "0                 0           0           0           0           0   \n",
      "1                 0           0           0           0           0   \n",
      "2                 0           0           0           0           0   \n",
      "3                 0           0           0           0           0   \n",
      "4                 0           0           0           0           0   \n",
      "...             ...         ...         ...         ...         ...   \n",
      "1799782           0           0           0           0           0   \n",
      "1799783           0           0           0           0           0   \n",
      "1799784           0           0           1           0           0   \n",
      "1799785           0           0           0           0           0   \n",
      "1799786           0           0           1           0           1   \n",
      "\n",
      "         Location_5  Location_6  Location_7  Location_8  Location_9  ...  \\\n",
      "0                 0           0           0           0           0  ...   \n",
      "1                 0           0           0           0           0  ...   \n",
      "2                 0           0           0           0           0  ...   \n",
      "3                 0           0           0           0           0  ...   \n",
      "4                 0           0           0           0           0  ...   \n",
      "...             ...         ...         ...         ...         ...  ...   \n",
      "1799782           0           0           0           0           0  ...   \n",
      "1799783           0           0           1           0           0  ...   \n",
      "1799784           1           1           1           1           1  ...   \n",
      "1799785           0           0           1           0           1  ...   \n",
      "1799786           0           0           0           0           0  ...   \n",
      "\n",
      "         Book-Title-90  Book-Title-91  Book-Title-92  Book-Title-93  \\\n",
      "0            -0.003576       0.006230       0.001085       0.000942   \n",
      "1            -0.003576       0.006230       0.001085       0.000942   \n",
      "2             0.000356      -0.004731      -0.031634      -0.000219   \n",
      "3             0.000356      -0.004731      -0.031634      -0.000219   \n",
      "4            -0.030692      -0.006653       0.018388      -0.015511   \n",
      "...                ...            ...            ...            ...   \n",
      "1799782       0.000110       0.001424       0.033407      -0.032764   \n",
      "1799783       0.000110       0.001424       0.033407      -0.032764   \n",
      "1799784       0.000110       0.001424       0.033407      -0.032764   \n",
      "1799785       0.000110       0.001424       0.033407      -0.032764   \n",
      "1799786       0.000110       0.001424       0.033407      -0.032764   \n",
      "\n",
      "         Book-Title-94  Book-Title-95  Book-Title-96  Book-Title-97  \\\n",
      "0             0.003243      -0.001159      -0.004393       0.000445   \n",
      "1             0.003243      -0.001159      -0.004393       0.000445   \n",
      "2            -0.011431      -0.029214      -0.013385      -0.012658   \n",
      "3            -0.011431      -0.029214      -0.013385      -0.012658   \n",
      "4             0.018370      -0.032536      -0.008342       0.019191   \n",
      "...                ...            ...            ...            ...   \n",
      "1799782       0.026947      -0.015558       0.001445      -0.007487   \n",
      "1799783       0.026947      -0.015558       0.001445      -0.007487   \n",
      "1799784       0.026947      -0.015558       0.001445      -0.007487   \n",
      "1799785       0.026947      -0.015558       0.001445      -0.007487   \n",
      "1799786       0.026947      -0.015558       0.001445      -0.007487   \n",
      "\n",
      "         Book-Title-98  Book-Title-99  \n",
      "0             0.003969      -0.000411  \n",
      "1             0.003969      -0.000411  \n",
      "2            -0.016783      -0.006358  \n",
      "3            -0.016783      -0.006358  \n",
      "4            -0.001306       0.004986  \n",
      "...                ...            ...  \n",
      "1799782       0.014254       0.004830  \n",
      "1799783       0.014254       0.004830  \n",
      "1799784       0.014254       0.004830  \n",
      "1799785       0.014254       0.004830  \n",
      "1799786       0.014254       0.004830  \n",
      "\n",
      "[1799787 rows x 139 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Split into train and test\n",
    "X, y = generateInput(usersDf, bookDf, ratingDf)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1065905    8\n",
       "1249048    0\n",
       "1547582    0\n",
       "436463     0\n",
       "464585     0\n",
       "          ..\n",
       "490951     0\n",
       "238508     0\n",
       "1222286    0\n",
       "86228      0\n",
       "1136330    0\n",
       "Name: Book-Rating, Length: 1439829, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "#tuneable parameters\n",
    "model = xgb.XGBRegressor(tree_method=\"hist\", learning_rate=0.1, max_depth=15, min_child_weight=5, n_estimators=250)\n",
    "model.fit(X_train, y_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/legoeuro/codeing/cs538/Book-Rec-System/.venv/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [03:59:28] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.10176289,  0.08312488,  1.0975707 , ...,  0.04397249,\n",
       "        1.4972544 , -0.05430675], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict (model, df):\n",
    "    return model.predict(df)\n",
    "  \n",
    "predictions = predict(model, X_test)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6241809822125899"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "predictions = pd.DataFrame(predictions)\n",
    "rms = sqrt(mean_squared_error(y_test, predictions))\n",
    "\n",
    "rms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
